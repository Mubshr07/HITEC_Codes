{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as mplot\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "import subprocess\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score,  precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.preprocessing import StandardScaler \n",
    " \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "\n",
    "import openpyxl\n",
    "import random\n",
    "  \n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist \n",
    " \n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.metrics import AUC, Precision, Recall, F1Score\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import pickle \n",
    "import shap \n",
    "from tabulate import tabulate \n",
    "\n",
    "# Define custom metrics\n",
    "recall = Recall()\n",
    "precision = Precision()\n",
    "f1_score = F1Score()\n",
    "auc = AUC()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetIndex = 6\n",
    "needToMakePictureOfTrees = 0\n",
    "''' ---------------------------------------------------------- '''\n",
    "dataSetFilePath = \"\"\n",
    "dataSetName = \"\"\n",
    "dataSetResultDirectory = \"./\"\n",
    "\n",
    "if(dataSetIndex == 0):\n",
    "    dataSetFilePath = \"./heartDisease/0_statLog_dataSet.csv\"\n",
    "    dataSetName = \"SateLog_DataSet\"\n",
    "elif (dataSetIndex == 1):\n",
    "    dataSetFilePath = \"./heartDisease/1_heart_statlog_cleveland_hungary_final.csv\"\n",
    "    dataSetName = \"ALL_StateLog_CleveLand_Hungary\"\n",
    "elif (dataSetIndex == 2):\n",
    "    dataSetFilePath = \"./heartDisease/2_cleveland.csv\"\n",
    "    dataSetName = \"Cleveland\"\n",
    "elif (dataSetIndex == 3):\n",
    "    dataSetFilePath = \"./heartDisease/3_framingham.csv\"\n",
    "    dataSetName = \"framingham\"\n",
    "elif (dataSetIndex == 4):\n",
    "    dataSetFilePath = \"./heartDisease/4_CardiacPrediction.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 5):\n",
    "    dataSetFilePath = \"./heartDisease/5_CardiacPredictionLessDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "elif (dataSetIndex == 6):\n",
    "    dataSetFilePath = \"./heartDisease/6_CardiacPredictionFewDimensions.xlsx\"\n",
    "    dataSetName = \"CardiacPrediction\"\n",
    "else:\n",
    "    dataSetFilePath = \"\"\n",
    "    dataSetName = \"\"\n",
    "\n",
    "if(dataSetIndex==4 or dataSetIndex==5 or dataSetIndex==6):\n",
    "    #fileData = pd.read_excel(dataSetFilePath, sheet_name='CoroHeartDis')\n",
    "    fileData = pd.read_excel(dataSetFilePath)\n",
    "else:\n",
    "    fileData = pd.read_csv(dataSetFilePath)\n",
    "\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))\n",
    "print(\"Column Headings: {}\".format(fileData.__dataframe__().column_names()))\n",
    "print(\"Number of Records: {}\".format(fileData.__dataframe__().num_rows()))\n",
    "\n",
    "\n",
    "missingValues = fileData.isnull().any().sum()\n",
    "print(f\"\\nNumber of Missing Values: {missingValues}\")\n",
    "\n",
    "num_rows_before = fileData.shape[0]\n",
    "# Remove duplicate records based on all columns\n",
    "fileData.drop_duplicates(inplace=True)\n",
    "# Check the number of rows after removing duplicates\n",
    "num_rows_after = fileData.shape[0]\n",
    "# Print the number of duplicate records removed\n",
    "num_duplicates_removed = num_rows_before - num_rows_after\n",
    "print(f\"Number of duplicate records removed: {num_duplicates_removed}\")\n",
    " \n",
    " # Preprocess Steps from the ChatGPT\n",
    "# 1. Handling Missing Values:\n",
    "fileData = fileData.dropna()\n",
    "print(\"Shape of fileData: {}\".format(fileData.shape))              \n",
    "#fileData.replace({'?': np.nan}).dropna().astype(float)\n",
    "#fileData = fileData.fillna(0) \n",
    "\n",
    "fileData = fileData.fillna(0) \n",
    "\n",
    "print(\"Shape of fileData End: {}\".format(fileData.shape))\n",
    "\n",
    "finalResultTable = [ ['Index', 'Method', 'Accuracy %','Recall %','Precision %','F1 Score','AUC'], ]  \n",
    "\n",
    "\n",
    "X = fileData.drop(fileData.__dataframe__().column_names()[-1], axis=1)  # Features\n",
    "X = X.drop(\"Gender\", axis=1)  \n",
    "#cols = ['Gender', 'Age','Annual-Family-Income', 'Cholesterol', 'Diabetes', 'Triglycerides', 'Red-Cell-Distribution-Width', 'X60-sec-pulse', 'Height', 'Albumin', 'Blood-Rel-Stroke', 'Blood-Rel-Diabetes', 'HDL', 'Moderate-work','Iron', 'Hemoglobin','Protein', 'SEQN'   ] \n",
    "#cols = ['Age','Gender','Blood-Rel-Stroke','Triglycerides','Blood-Rel-Diabetes','Cholesterol','Platelet-count','Diabetes','Albumin','Hemoglobin','Moderate-work','Diastolic','Protein','Height','X60-sec-pulse','White-Blood-Cells','Bilirubin','Hematocrit','HDL','Systolic' ] \n",
    "#X = fileData[cols]\n",
    "\n",
    "Y = fileData[fileData.__dataframe__().column_names()[-1]]  # Labels\n",
    "\n",
    "columns = fileData.__dataframe__().column_names() \n",
    "totalRecords = (fileData.__dataframe__().num_rows())\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"columns of x:: {} \\n\\n and features of X: {}\".format(len(X.columns), X.columns))\n",
    "\n",
    "dataSetResultDirectory = \"./\"\n",
    "dataSetResultDirectory += (\"DatasetResults_MLP_imBalance_SHAP_Final\")\n",
    "dataSetResultDirectory += \"/\"\n",
    "if not os.path.isdir(dataSetResultDirectory):\n",
    "    os.makedirs(dataSetResultDirectory)\n",
    "\n",
    "dataSetName += \"_{}\".format(fileData.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of fileData: {} , target Len:{}\".format(fileData.shape, len(Y)))\n",
    "print(\"X: {} , Y:{}\".format(X.shape, Y.shape))\n",
    "#print(\"\\n\\nX: head:: \\n{}\".format(X.head()))\n",
    "#print(\"\\n\\nY: head::\\n {}\".format(Y.head()))\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=42)\n",
    "  \n",
    "print(\"\\n X Train: Shape:: {}\".format(X_train.shape))\n",
    "print(\" X Test: Shape:: {}\".format(X_test.shape))  \n",
    "  \n",
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_train:\n",
    "    if(i == 0):\n",
    "        negativeClass += 1\n",
    "    if(i == 1):\n",
    "        positiveClass += 1\n",
    "print(\"Train DataSet Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\"Train DataSet Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\"Train DataSet Total Records:: {}\".format(positiveClass + negativeClass)) \n",
    "print(\"\\n\\n\") \n",
    "\n",
    "positiveClass =  0\n",
    "negativeClass = 0\n",
    "for i in y_test:\n",
    "    if(i == 0):\n",
    "        negativeClass += 1\n",
    "    if(i == 1):\n",
    "        positiveClass += 1\n",
    "print(\"Test DataSet Positive Class Records:: {}\".format(positiveClass)) \n",
    "print(\"Test DataSet Negative Class Records:: {}\".format(negativeClass)) \n",
    "print(\"Test DataSet Total Records:: {}\".format(positiveClass + negativeClass)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeConfusionMatrixPic(method, dataSet, classifierObj , X_test, y_test, predicted_Y):\n",
    "    display = ConfusionMatrixDisplay.from_estimator(classifierObj, X_test, y_test, display_labels=['Healthy', \"Heart Disease\"], cmap=mplot.cm.Blues) #, normalize=\"true\"\n",
    "    display.ax_.set_title(\"Confusion Matrix ({} Model)\".format(method))\n",
    "    ax_.set_xlabel('\\nPredicted Values')\n",
    "    ax_.set_ylabel('Actual Values ')\n",
    "\n",
    "\n",
    "    accuracyString =\"Accuracy {}: {:.2f}\".format(method, accuracy_score(y_test, predicted_Y)*100.0 ) \n",
    "    recallString =  'Recall {}: {:.2f}'.format(method, recall_score(y_test, predicted_Y) * 100.0)\n",
    "    precisionString = 'Precision {}: {:.2f}'.format(method, precision_score(y_test, predicted_Y) * 100.0) \n",
    "    dataSetString = \"Dataset: {}\".format(dataSet)\n",
    "\n",
    "    \n",
    "    if(classifierObj.n_features_in_ > 10):\n",
    "        featureListString = 'Total Features: {}'.format(classifierObj.n_features_in_) \n",
    "    else:\n",
    "        featureListString = 'Features: {}'.format(classifierObj.feature_names_in_) \n",
    "    \n",
    "    display.figure_.text(0.010, -0.05,  accuracyString, horizontalalignment='left', wrap=False )  \n",
    "    display.figure_.text(0.010, -0.09,  recallString, horizontalalignment='left', wrap=False )      \n",
    "    display.figure_.text(0.010, -0.13,  precisionString, horizontalalignment='left', wrap=False ) \n",
    "    display.figure_.text(0.010, -0.17,  dataSetString, horizontalalignment='left', wrap=False ) \n",
    "    display.figure_.text(0.010, -0.28,  featureListString, horizontalalignment='left', wrap=False ) \n",
    " \n",
    "    picturePath = \"{}Confusion_Matrix_{}_{}.png\".format(dataSetResultDirectory, method, dataSetName)\n",
    "    mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #print(\"{} Confusion Matrix saved:: path: {}\".format(method, picturePath))\n",
    "    #os.startfile(picturePath)\n",
    "    mplot.show()\n",
    "    mplot.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normalized = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_test_normalized = tf.keras.utils.normalize(X_test, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaler = scaler.fit_transform(X_train)\n",
    "X_test_scaler = scaler.fit_transform(X_test) \n",
    "# Our vectorized labels\n",
    "\n",
    "X_train_f32 = np.asarray(X_train).astype(np.float32)  #.astype('float32').reshape((-1,1))\n",
    "X_test_f32 = np.asarray(X_test).astype(np.float32)\n",
    "\n",
    "#y_train_scaler = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "y_test_scaler = np.asarray(y_test).astype('float32').reshape((-1,1))\n",
    "\n",
    "# Separate features and target variable\n",
    "features = X_train_scaler # data.iloc[:, :-1]\n",
    "target = np.asarray(y_train).astype('float64').reshape((-1,1))  #data['CoronaryHeartDisease']\n",
    " \n",
    "print(\"X_train shape: {}   and dType: {}\".format(X_train.shape, len(X_train.columns)))\n",
    "print(\"X_train_scaler shape: {}   and dType: {}\".format(X_train_scaler.shape, X_train_scaler.dtype))\n",
    "print(\"X_test_scaler shape: {}   and dType: {}\".format(X_test_scaler.shape, X_test_scaler.dtype)) \n",
    "\n",
    "print(\"y_train shape: {}   and dType: {}\".format(y_train.shape, y_train.dtype))  \n",
    "print(\"y_test_scaler shape: {}   and dType: {}\".format(y_test_scaler.shape, y_test_scaler.dtype))  \n",
    "\n",
    "print(\"features shape: {}   and dType: {}\".format(features.shape, features.dtype))\n",
    "print(\"target shape: {}   and dType: {}\".format(target.shape, target.dtype)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()   \n",
    "totalFeatures= features.shape[1] \n",
    "print(\"Total Feature for number of node in Layer: {}\".format(totalFeatures)) \n",
    "input_layer = Input(shape=(totalFeatures,))\n",
    "# Traditional neural network part\n",
    "x = layers.Dense(int(totalFeatures), activation='relu')(input_layer)\n",
    "x = layers.Dense(int(totalFeatures*0.75), activation='relu')(x)\n",
    "x = layers.Dense(int(totalFeatures*0.50), activation='relu')(x)\n",
    "x = layers.Dense(int(totalFeatures*0.25), activation='relu')(x) \n",
    "\n",
    "output_layer = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=input_layer, outputs=output_layer) \n",
    "optimizer = AdamW(learning_rate=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy', Recall(), Precision(), AUC()])  # Add custom metrics\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfEpochs = 32\n",
    "batchSizeOfTraining = 25\n",
    "history = 0\n",
    "history = model.fit(features, target, epochs=numberOfEpochs, batch_size=batchSizeOfTraining)\n",
    "\n",
    "model2 = model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting training accuracy\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], marker='o', linestyle='-', color='b')\n",
    "plt.title('Training Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "picturePath = \"{}3.Model_training_Accuracy_{}_epoches_{}.png\".format(dataSetResultDirectory, dataSetName, numberOfEpochs)\n",
    "plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "method = \"MLP\"\n",
    "# Plot Testing accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "for metric_name in history.history.keys():\n",
    "    plt.plot(history.history[metric_name], label=metric_name.capitalize())\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('Model Training Metrics')\n",
    "# Add legend\n",
    "plt.legend()\n",
    "picturePath = \"{}Model_Training_Evaluation_{}_{}_Epoch_{}.png\".format(dataSetResultDirectory, method, dataSetName, numberOfEpochs)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Evaluate the model using the evaluate method\n",
    "y_test_float64 = np.asarray(y_test).astype('float64').reshape((-1,1))  #data['CoronaryHeartDisease']\n",
    "loss, accuracy, recall_value, precision_value,  auc_value = model2.evaluate(X_test_scaler, y_test_float64)\n",
    "\n",
    "# Print the results\n",
    "print('Test loss: {}'.format(loss*100))\n",
    "print('Test accuracy: {}'.format(accuracy*100))\n",
    "print('Test recall: {}'.format(recall_value*100))\n",
    "print('Test precision: {}'.format(precision_value*100))\n",
    "print('Test AUC: {}'.format(auc_value*100)) \n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "threshold = 0.5  # Adjust the threshold as needed\n",
    "binary_predictions = (model2.predict(X_test_scaler) > threshold).astype(int)\n",
    "cm = confusion_matrix(y_test_float64, binary_predictions)\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Healthy', 'Heart Disease'])\n",
    "display.plot(cmap=plt.cm.Blues, values_format=\".4g\" ) \n",
    "display.ax_.set_title(\"Results {} Model\".format(method),fontsize=16, fontweight='bold')\n",
    "display.ax_.set_xlabel('\\nPredicted Values')\n",
    "display.ax_.set_ylabel('Actual Values ')\n",
    "\n",
    "accuracyString =\"Accuracy {}: {:.2f}%\".format(method, accuracy*100.0 ) \n",
    "recallString =  'Recall {}: {:.2f}%'.format(method, recall_value* 100.0)\n",
    "precisionString = 'Precision {}: {:.2f}%'.format(method, precision_value * 100.0) \n",
    "#f1String = \"F1 Score: {:.2f}\".format(f1_score_value[0] * 100.0)\n",
    "featureListString = \"AUC Score: {:.2f}%\".format(auc_value * 100.0)\n",
    "totaldataSetString = \"Total Dataset records: {}\".format((len(features)+len(X_test_scaler)))\n",
    "testingdataSetString = \"Testing records: {} , {:.1f}%\".format(len(X_test_scaler), ( ( len(X_test_scaler) / (len(features)+len(X_test_scaler)) )*100.0  ))\n",
    "numberOfEpochsString = \"Number of Epoches: {}\".format(numberOfEpochs)\n",
    "batchSizeOfTrainingString = \"BatchSize for Epoch: {}\".format(batchSizeOfTraining)\n",
    "\n",
    "display.figure_.text(0.010, -0.05,  accuracyString, horizontalalignment='left', wrap=False )  \n",
    "display.figure_.text(0.010, -0.09,  recallString, horizontalalignment='left', wrap=False )      \n",
    "display.figure_.text(0.010, -0.13,  precisionString, horizontalalignment='left', wrap=False ) \n",
    "#display.figure_.text(0.010, -0.17,  f1String, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.21,  featureListString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.26,  totaldataSetString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.30,  testingdataSetString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.34,  numberOfEpochsString, horizontalalignment='left', wrap=False ) \n",
    "display.figure_.text(0.010, -0.38,  batchSizeOfTrainingString, horizontalalignment='left', wrap=False ) \n",
    " \n",
    "picturePath = \"{}Model_Evaluation_{}_{}_Epoch_{}.png\".format(dataSetResultDirectory, method, dataSetName, numberOfEpochs)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#print(\"{} Confusion Matrix saved:: path: {}\".format(method, picturePath))\n",
    "mplot.show()\n",
    "mplot.close()\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "# Calculate AUC\n",
    "auc_score = roc_auc_score(y_test, binary_predictions)\n",
    "# Calculate ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test, binary_predictions)\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = {:.2f})'.format(auc_score))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "picturePath = \"{}Model_Evaluation_ROC_{}_{}_Epoch_{}.png\".format(dataSetResultDirectory, method, dataSetName, numberOfEpochs)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------\n",
    "csvPath = \"{}Model_training_accuracy_and_evaluations_{}_{}_Epoch_{}.xlsx\".format(dataSetResultDirectory, method, dataSetName, numberOfEpochs)\n",
    "# Create a new workbook\n",
    "wb = openpyxl.Workbook()\n",
    "ws = wb.active  # Get the active worksheet\n",
    "str22 = (classification_report(y_test, binary_predictions)) \n",
    "str22.strip() \n",
    "str22.replace(\" \", \",\")\n",
    "#print(str22)\n",
    "print(\"-------------=-=-=-=----------------\")\n",
    "data_lines = str22.splitlines()  # Split by newlines\n",
    "str22 = str(\"\\n\\n\\n Training accuracy:\\n\"  )\n",
    "data_lines += str22.splitlines()  # Split by newlines\n",
    "str22 = str(history.history['accuracy'])\n",
    "str22.strip() \n",
    "str22.replace(\" \", \",\")\n",
    "str22.replace(\"[\", \"\")\n",
    "str22.replace(\"]\", \"\")\n",
    "#print(str22)\n",
    "data_lines += str22.splitlines()  # Split by newlines\n",
    "\n",
    "\n",
    "str22 = str(\"\\n\\n\\n ROC Values:\\n\"  )\n",
    "data_lines += str22.splitlines()  # Split by newlines\n",
    "str22 = str(\"False Positive Rate:, {}\".format(fpr))\n",
    "data_lines += str22.splitlines()  # Split by newlines\n",
    "str22 = str(\"True Positive Rate:, {}\".format(tpr))\n",
    "data_lines += str22.splitlines()  # Split by newlines\n",
    "str22 = str(\"ROC curve, (AUC = {})\".format(auc_score))\n",
    "data_lines += str22.splitlines()  # Split by newlines\n",
    "\n",
    "\n",
    "\n",
    "print(data_lines)\n",
    "# Split each line into a list (comma-separated values)\n",
    "xlsFileData = [line.split(\",\") for line in data_lines]\n",
    "# Write the data to the worksheet, starting from row 1\n",
    "for row_index, row in enumerate(xlsFileData):\n",
    "    for col_index, value in enumerate(row):\n",
    "        ws.cell(row=row_index + 1, column=col_index + 1).value = value\n",
    "\n",
    "# Save the workbook\n",
    "wb.save(csvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24erwer234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage = 0.03 \n",
    "featuresForShap = X_train.columns #features[0:numberOfFeatures]\n",
    "#print(\" Features Name: {}\".format(  featuresForShap))\n",
    " \n",
    "testForShap = X_test_scaler[0:500]\n",
    "#print(\" testForShap Name: {}\".format(  testForShap))\n",
    "print(\"TensorFlow Version: {}\".format(tf.__version__))\n",
    "print(\"SHAP Version: {}\".format(shap.__version__))\n",
    "\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_explainer = shap.Explainer(model, feature_names=featuresForShap, masker=shap.maskers.Independent(data=testForShap)) \n",
    "shap_values = shap_explainer.shap_values(testForShap)  \n",
    "\n",
    "# Calculate feature importance based on the mean absolute SHAP values\n",
    "feature_names = X_train.columns.tolist()\n",
    "# Calculate average SHAP values across all instances\n",
    "avg_shap_values = np.mean(shap_values, axis=0) \n",
    "simpleExplainerShap_array = np.array(shap_values) \n",
    "mean_abs_simpleShap_values = np.mean(np.abs(simpleExplainerShap_array), axis=(0)) \n",
    "sorted_indices = np.argsort(-mean_abs_simpleShap_values) #[::-1] \n",
    "sorted_feature_names = np.array(X.columns.to_list())[sorted_indices]\n",
    "sorted_shap_values = simpleExplainerShap_array[:, sorted_indices].T\n",
    "#print(sorted_shap_values) \n",
    "deepShapValuesPlot = mean_abs_simpleShap_values[sorted_indices]\n",
    "print(\"Feature Ranking:\")\n",
    "index = 1\n",
    "for feature, importance in zip(sorted_feature_names, mean_abs_simpleShap_values[sorted_indices]): \n",
    "  print(\"{} , {}, {}\".format(index, feature, importance))\n",
    "  index = index+1\n",
    "#------------------------------------------------------------------------------------------- \n",
    "howManyFeatures = 37\n",
    "# Calculate feature importance based on the mean absolute SHAP values\n",
    "feature_importance =  np.abs(shap_values).mean(axis=0)\n",
    "top_features_indices = np.argsort(feature_importance)[::-1][:howManyFeatures]\n",
    "# Select only the top features and corresponding SHAP values\n",
    "print(top_features_indices)\n",
    "featureNamesSHAP = X.columns[top_features_indices]\n",
    "top_features = testForShap[:, top_features_indices]\n",
    "top_shap_values = shap_values[:, top_features_indices]\n",
    "print(\"\\n\\n--------------------------------------------------\") \n",
    "print(\"Top SHAP Explainer values:\")\n",
    "for i in range(len(top_features_indices)):\n",
    "    feature_index = top_features_indices[i]\n",
    "    feature_name = feature_names[feature_index]\n",
    "    shap_value = np.mean(np.abs(top_shap_values[:, i])) \n",
    "    #print(f\"{feature_name}, {shap_value}\")\n",
    "#---------------------------------------------------------------------------------\n",
    "str22 = 0\n",
    "csvPath = \"{}Model_SHAP_simpleExplainer_FeatureRanking_{}_{}_Epoch_{}.xlsx\".format(dataSetResultDirectory, method, dataSetName, numberOfEpochs)\n",
    "str22 = str(\"SimpleExplainer Top Feature List\\n\")\n",
    "indexx = 1\n",
    "for feature, importance in zip(sorted_feature_names, mean_abs_simpleShap_values[sorted_indices]): \n",
    "        str22 += str(\"{}, {}, {} \\n\".format(indexx, feature,importance)) \n",
    "        indexx += 1\n",
    "wb = openpyxl.Workbook()\n",
    "ws = wb.active  # Get the active worksheet\n",
    "data_lines = str22.splitlines()  # Split by newlines\n",
    "xlsFileData = [line.split(\",\") for line in data_lines] \n",
    "for row_index, row in enumerate(xlsFileData):\n",
    "    for col_index, value in enumerate(row):\n",
    "        ws.cell(row=row_index + 1, column=col_index + 1).value = value\n",
    "wb.save(csvPath)\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "howManyFeatures = 10\n",
    "# Calculate feature importance based on the mean absolute SHAP values\n",
    "feature_importance =  np.abs(shap_values).mean(axis=0)\n",
    "top_features_indices = np.argsort(feature_importance)[::-1][:howManyFeatures]\n",
    "# Select only the top features and corresponding SHAP values\n",
    "print(top_features_indices)\n",
    "featureNamesSHAP = X.columns[top_features_indices]\n",
    "top_features = testForShap[:, top_features_indices]\n",
    "top_shap_values = shap_values[:, top_features_indices]\n",
    "# Plot the summary plot for the top 15 features\n",
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, plot_type=\"bar\", show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "shapTypeString =\"SHAP Type: {}\".format(repr(shap_explainer)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "ax.figure.text(0.020, -0.13,  shapTypeString, horizontalalignment='left', wrap=False )   \n",
    "picturePath = \"{}XAI_SHAP_Explainer_Bar_{}_{}_Sorted_numberOfSamples_{}_Features_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap),howManyFeatures)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()\n",
    "# Plot the summary plot for the top 15 features\n",
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "shapTypeString =\"SHAP Type: {}\".format(repr(shap_explainer)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "ax.figure.text(0.020, -0.13,  shapTypeString, horizontalalignment='left', wrap=False )   \n",
    "picturePath = \"{}XAI_SHAP_Explainer_SummaryPlot_{}_{}_Sorted_numberOfSamples_{}_Features_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap),howManyFeatures)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()\n",
    "\n",
    "howManyFeatures = 15\n",
    "# Calculate feature importance based on the mean absolute SHAP values\n",
    "feature_importance =  np.abs(shap_values).mean(axis=0)\n",
    "top_features_indices = np.argsort(feature_importance)[::-1][:howManyFeatures]\n",
    "# Select only the top features and corresponding SHAP values\n",
    "print(top_features_indices)\n",
    "featureNamesSHAP = X.columns[top_features_indices]\n",
    "top_features = testForShap[:, top_features_indices]\n",
    "top_shap_values = shap_values[:, top_features_indices]\n",
    "# Plot the summary plot for the top 15 features\n",
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, plot_type=\"bar\", show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "shapTypeString =\"SHAP Type: {}\".format(repr(shap_explainer)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "ax.figure.text(0.020, -0.13,  shapTypeString, horizontalalignment='left', wrap=False )   \n",
    "picturePath = \"{}XAI_SHAP_Explainer_Bar_{}_{}_Sorted_numberOfSamples_{}_Features_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap),howManyFeatures)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()\n",
    "# Plot the summary plot for the top 15 features\n",
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "shapTypeString =\"SHAP Type: {}\".format(repr(shap_explainer)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "ax.figure.text(0.020, -0.13,  shapTypeString, horizontalalignment='left', wrap=False )   \n",
    "picturePath = \"{}XAI_SHAP_Explainer_SummaryPlot_{}_{}_Sorted_numberOfSamples_{}_Features_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap),howManyFeatures)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()\n",
    "\n",
    "\n",
    "\n",
    "howManyFeatures = 20\n",
    "# Calculate feature importance based on the mean absolute SHAP values\n",
    "feature_importance =  np.abs(shap_values).mean(axis=0)\n",
    "top_features_indices = np.argsort(feature_importance)[::-1][:howManyFeatures]\n",
    "# Select only the top features and corresponding SHAP values\n",
    "print(top_features_indices)\n",
    "featureNamesSHAP = X.columns[top_features_indices]\n",
    "top_features = testForShap[:, top_features_indices]\n",
    "top_shap_values = shap_values[:, top_features_indices]\n",
    "# Plot the summary plot for the top 15 features\n",
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, plot_type=\"bar\", show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "shapTypeString =\"SHAP Type: {}\".format(repr(shap_explainer)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "ax.figure.text(0.020, -0.13,  shapTypeString, horizontalalignment='left', wrap=False )   \n",
    "picturePath = \"{}XAI_SHAP_Explainer_Bar_{}_{}_Sorted_numberOfSamples_{}_Features_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap),howManyFeatures)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()\n",
    "# Plot the summary plot for the top 15 features\n",
    "shap.summary_plot(top_shap_values, top_features, feature_names=featureNamesSHAP, show=False)\n",
    "ax = mplot.gca() \n",
    "ax.set_title(\"XAI SHAP Explainer Sorted  ({} Features)\".format(howManyFeatures) ,fontsize=16, fontweight='bold')     \n",
    "dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "shapTypeString =\"SHAP Type: {}\".format(repr(shap_explainer)) \n",
    "ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "ax.figure.text(0.020, -0.13,  shapTypeString, horizontalalignment='left', wrap=False )   \n",
    "picturePath = \"{}XAI_SHAP_Explainer_SummaryPlot_{}_{}_Sorted_numberOfSamples_{}_Features_{}.png\".format(dataSetResultDirectory, method, dataSetName, len(testForShap),howManyFeatures)\n",
    "mplot.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "#mplot.show()\n",
    "#os.startfile(picturePath)\n",
    "mplot.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepExplainer = shap.Explainer(model, features[0:int(1000)], feature_names=featuresForShap)\n",
    "#deepExplainer = shap.Explainer(model)\n",
    "deepTestValues =  testForShap[0:int(400)] \n",
    "shap_FitData = deepExplainer(deepTestValues)\n",
    "\n",
    "print(shap_FitData.shape) #output: (4177, 8)\n",
    "howManyWaterFallPlotsRequired = int(len(shap_FitData)//2.0)\n",
    "print(\"\\nHow many Waterfall plots are generated: {}\".format(howManyWaterFallPlotsRequired))\n",
    "\n",
    "# Water fall Plots\n",
    "displayNumberOfFeatures = 20\n",
    "shapDirectory = \"\"\n",
    "shapDirectory += dataSetResultDirectory + (\"SHAP_WaterFall_Epoches_{}_Features_{}\".format(numberOfEpochs, displayNumberOfFeatures))\n",
    "shapDirectory += \"/\"\n",
    "if not os.path.isdir(shapDirectory):\n",
    "    os.makedirs(shapDirectory)\n",
    "index = 1\n",
    "for sample in shap_FitData[0:howManyWaterFallPlotsRequired]:\n",
    "    plt.figure(figsize=(8, 16))\n",
    "    plt.switch_backend('Agg')\n",
    "    shap.waterfall_plot(sample, max_display=displayNumberOfFeatures)\n",
    "    ax = mplot.gca() \n",
    "    ax.set_title(\"XAI SHAP Waterfall Plot of Sample {}\".format(index) ,fontsize=16, fontweight='bold')     \n",
    "    dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "    testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "    ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "    ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "    picturePath = \"{}XAI_SHAP_Explainer_WaterFall_{}_TotalSHAPvalues_{}_Epoch_{}.png\".format(shapDirectory, index,  len(testForShap), numberOfEpochs) \n",
    "    plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #mplot.show()\n",
    "    #os.startfile(picturePath)\n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    index = index+1\n",
    "\n",
    "\n",
    "displayNumberOfFeatures = 15\n",
    "shapDirectory = \"\"\n",
    "shapDirectory += dataSetResultDirectory + (\"SHAP_WaterFall_Epoches_{}_Features_{}\".format(numberOfEpochs, displayNumberOfFeatures))\n",
    "shapDirectory += \"/\"\n",
    "if not os.path.isdir(shapDirectory):\n",
    "    os.makedirs(shapDirectory)\n",
    "index = 1\n",
    "for sample in shap_FitData[0:howManyWaterFallPlotsRequired]:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.switch_backend('Agg')\n",
    "    shap.waterfall_plot(sample, max_display=displayNumberOfFeatures)\n",
    "    ax = mplot.gca() \n",
    "    ax.set_title(\"XAI SHAP Waterfall Plot of Sample {}\".format(index) ,fontsize=16, fontweight='bold')     \n",
    "    dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "    testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "    ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "    ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "    picturePath = \"{}XAI_SHAP_Explainer_WaterFall_{}_TotalSHAPvalues_{}_Epoch_{}.png\".format(shapDirectory, index,  len(testForShap), numberOfEpochs) \n",
    "    plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #mplot.show()\n",
    "    #os.startfile(picturePath)\n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    index = index+1\n",
    "\n",
    "\n",
    "displayNumberOfFeatures = 10\n",
    "shapDirectory = \"\"\n",
    "shapDirectory += dataSetResultDirectory + (\"SHAP_WaterFall_Epoches_{}_Features_{}\".format(numberOfEpochs, displayNumberOfFeatures))\n",
    "shapDirectory += \"/\"\n",
    "if not os.path.isdir(shapDirectory):\n",
    "    os.makedirs(shapDirectory)\n",
    "index = 1\n",
    "for sample in shap_FitData[0:howManyWaterFallPlotsRequired]:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.switch_backend('Agg')\n",
    "    shap.waterfall_plot(sample, max_display=displayNumberOfFeatures)\n",
    "    ax = mplot.gca() \n",
    "    ax.set_title(\"XAI SHAP Waterfall Plot of Sample {}\".format(index) ,fontsize=16, fontweight='bold')     \n",
    "    dataSetString = \"Dataset:  {}\".format(dataSetName)\n",
    "    testingDatasetString =\"length of SHAP dataset: {}\".format(len(testForShap))\n",
    "    ax.figure.text(0.020, -0.05,  dataSetString, horizontalalignment='left', wrap=False )  \n",
    "    ax.figure.text(0.020, -0.09,  testingDatasetString, horizontalalignment='left', wrap=False )   \n",
    "    picturePath = \"{}XAI_SHAP_Explainer_WaterFall_{}_TotalSHAPvalues_{}_Epoch_{}.png\".format(shapDirectory, index,  len(testForShap), numberOfEpochs) \n",
    "    plt.savefig(picturePath,  dpi=300, bbox_inches='tight')\n",
    "    #mplot.show()\n",
    "    #os.startfile(picturePath)\n",
    "    plt.close()\n",
    "    plt.clf()\n",
    "    index = index+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
